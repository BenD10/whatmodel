---
description: How to add or edit GPUs and models in the JSON data files
globs: src/lib/data/*.json
alwaysApply: false
---

# GPU and Model Data

Data lives in `src/lib/data/` as static JSON, imported at build time. No runtime fetching.

## Adding a GPU (`gpus.json`)

Entries are grouped by manufacturer (NVIDIA, AMD, Intel, Apple), sorted newest-first within each group.

Discrete GPU:
```json
{ "id": "rtx-5090", "name": "GeForce RTX 5090", "manufacturer": "NVIDIA", "vram_gb": 32, "bandwidth_gbps": 1792 }
```

Apple Silicon (multiple memory configs with different bandwidths):
```json
{
  "id": "m4-max",
  "name": "M4 Max",
  "manufacturer": "Apple",
  "vram_options": [
    { "vram_gb": 36, "bandwidth_gbps": 410 },
    { "vram_gb": 64, "bandwidth_gbps": 546 }
  ]
}
```

- `id`: kebab-case unique identifier (used in URL params)
- `manufacturer`: one of `"NVIDIA"`, `"AMD"`, `"Intel"`, `"Apple"` — used for dropdown grouping
- Discrete GPUs have `vram_gb` + `bandwidth_gbps` at top level
- Apple Silicon has `vram_options` array instead (no top-level `vram_gb`/`bandwidth_gbps`)

## Adding a Model (`models.json`)

Each entry represents one model at one quantization level:
```json
{
  "id": "llama-3.1-8b-q4",
  "name": "Llama 3.1 8B",
  "params_b": 8.03,
  "quantization": "Q4_K_M",
  "weight_gb": 4.92,
  "kv_per_1k_gb": 0.122,
  "max_context_k": 128,
  "mmlu_score": 69.4,
  "notes": "32 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
}
```

Required fields:
- `weight_gb`: actual GGUF file size from HuggingFace (bartowski repos)
- `kv_per_1k_gb`: `2 × layers × kv_heads × head_dim × 2 × 1000 / (1024³)` (fp16 KV cache)
- `max_context_k`: from model card (e.g. 128 for 128K context)
- `mmlu_score`: MMLU benchmark score (used for sorting and quality tier badges)
- `notes`: include architecture details (layers, KV heads, head_dim) and weight source

Models are grouped by family, with multiple quantization variants per model (typically Q4_K_M, Q8_0, fp16).
